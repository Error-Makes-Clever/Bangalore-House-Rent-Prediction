{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4114015a-d3ff-45cc-8b23-b867c7ee2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc5d80-c58b-477e-bb5b-b45ab9c115c5",
   "metadata": {},
   "source": [
    "### Description:\n",
    "This script scrapes rental property listings from 99acres.com for Bangalore Regions. \n",
    "It automates the browser using Selenium, extracts data with BeautifulSoup, \n",
    "and saves the scraped details in Excel files.\n",
    "\n",
    "### Key functionalities:\n",
    "- Uses a headless Chrome browser for scraping.\n",
    "- Iterates over multiple pages within a specified range.\n",
    "- Extracts various property details (e.g., area, rent, furnishing, location).\n",
    "- Handles missing data gracefully with a `safe_extract` function.\n",
    "- Saves processed data in Excel format per page.\n",
    "- Maintains a separate log for unprocessed property links.\n",
    "- Introduces random delays to mimic human behavior and avoid detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a541a2-a15c-4182-a771-7d56934c55bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_99acres(start_page, end_page):\n",
    "    \n",
    "    folder_name = \"99acres_Bangalore_Central_scraped_data\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    unprocessed_file = os.path.join(folder_name, \"99acres_unprocessed.xlsx\")\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.0.0 Safari/537.36\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    unprocessed_links = []\n",
    "\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        page_data = [] \n",
    "\n",
    "        try:\n",
    "            print(f\"Scraping page {page_number}...\")\n",
    "\n",
    "            url = f'https://www.99acres.com/property-for-rent-in-bangalore-central-ffid-page-{page_number}'\n",
    "            driver.get(url)\n",
    "\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"html\")))\n",
    "            time.sleep(random.uniform(5, 10)) \n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            links = [a.get(\"href\") for a in soup.find_all('a', class_=\"tupleNew__propertyHeading ellipsis\")]\n",
    "\n",
    "            if not links:\n",
    "                print(f\"No properties found on page {page_number}. Adding to unprocessed pages.\")\n",
    "                continue\n",
    "\n",
    "            for link in links:\n",
    "                try:\n",
    "                    driver.get(link)\n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"html\")))\n",
    "                    time.sleep(random.uniform(4, 7))\n",
    "\n",
    "                    new_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "                    def safe_extract(soup, tag, attr_type, attr_value):\n",
    "                        try:\n",
    "                            if attr_type == \"id\":\n",
    "                                element = soup.find(tag, id=attr_value)\n",
    "                            elif attr_type == \"class\":\n",
    "                                element = soup.find(tag, class_=attr_value)\n",
    "                            elif attr_type == \"all_class\":\n",
    "                                elements = soup.find_all(tag, class_=attr_value)\n",
    "                                return [item.text.strip() for item in elements]\n",
    "                            return element.text.strip() if element else np.nan\n",
    "                        except:\n",
    "                            return np.nan\n",
    "\n",
    "                    area_tags = ['superArea_span', 'superbuiltupArea_span', 'builtupArea_span', 'carpetArea_span']\n",
    "                    area = ''\n",
    "                    for tag in area_tags:\n",
    "                        area = safe_extract(new_soup, 'span', 'id', tag)\n",
    "                        if (area != '') & (pd.notna(area)):\n",
    "                            break\n",
    "\n",
    "                    available_from = new_soup.find_all('div' , class_ = 'component__details')\n",
    "\n",
    "                    floor = safe_extract(new_soup, 'span', 'id', 'Floor_Num_Label')\n",
    "\n",
    "                    page_data.append({\n",
    "                        \"Link\": link,\n",
    "                        \"Bedroom\": safe_extract(new_soup, 'span', 'id', 'bedRoomNum'),\n",
    "                        \"Bathroom\": safe_extract(new_soup, 'span', 'id', 'bathroomNum'),\n",
    "                        \"Balcony\": safe_extract(new_soup, 'span', 'id', 'balconyNum'),\n",
    "                        \"Additional_rooms\": safe_extract(new_soup, 'span', 'id', 'additionalRooms'),\n",
    "                        \"Area\": area,\n",
    "                        \"Facing\": safe_extract(new_soup, 'span', 'id', 'Facing_Label'),\n",
    "                        \"Furnishing\": safe_extract(new_soup, 'span', 'id', 'furnishingLabel'),\n",
    "                        \"Rating\": safe_extract(new_soup, 'div', 'class', 'display_l_semiBold'),\n",
    "                        \"Address\": safe_extract(new_soup, 'span', 'class', 'component__pdPropAddress'),\n",
    "                        \"Nearby\": safe_extract(new_soup, 'span', 'all_class', 'NearByLocation__infoText'),\n",
    "                        \"Power_backup\": safe_extract(new_soup, 'span', 'id', 'Powerbackup_Label'),\n",
    "                        \"Parking\": safe_extract(new_soup, 'span', 'id', 'Reserved_Parking_Label'),\n",
    "                        \"Charges\": safe_extract(new_soup, 'span', 'id', 'electricityWaterCharges'),\n",
    "                        \"Posted_By_and_On\" : safe_extract(new_soup,'span' , 'id' , 'postedOnAndByLabel'),\n",
    "                        \"Age\": safe_extract(new_soup, 'span', 'id', 'Age_Label'),\n",
    "                        \"Pet_friendly\": safe_extract(new_soup, 'span', 'id', 'PetFriendly'),\n",
    "                        \"Floor\": safe_extract(new_soup , 'span' , 'id' , 'Total_Floor') if pd.isnull(floor) else floor,\n",
    "                        \"Avaliable_from\" : available_from[len(available_from)-2].text.strip(),\n",
    "                        \"Available_for\": safe_extract(new_soup, 'span', 'id', 'availableForLabel'),\n",
    "                        \"Type\": safe_extract(new_soup, 'div', 'class', 'component__pdPropConfSide component__rentHeading pd__rentHeading'),\n",
    "                        \"Advance\": safe_extract(new_soup, 'div', 'class', 'component__tableTooltip'),\n",
    "                        \"Location\": safe_extract(new_soup, 'span', 'class', 'component__pdPropAddress'),\n",
    "                        \"Rent\": safe_extract(new_soup, 'span', 'id', 'pdPrice2')\n",
    "                    })\n",
    "\n",
    "                    print(f\"Scraped property: {link}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing link {link}: {e}\")\n",
    "                    unprocessed_links.append({\"Page\": page_number, \"Link\": link})\n",
    "                    continue\n",
    "\n",
    "                time.sleep(random.uniform(5, 8))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping page {page_number}: {e}\")\n",
    "            continue\n",
    "\n",
    "        df_page = pd.DataFrame(page_data)\n",
    "\n",
    "        file_name = os.path.join(folder_name, f\"99acres_rentals_page_{page_number}.xlsx\")\n",
    "        df_page.to_excel(file_name, index=False)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df_unprocessed_links = pd.DataFrame(unprocessed_links)\n",
    "\n",
    "    if os.path.exists(unprocessed_file):\n",
    "        existing_data = pd.ExcelFile(unprocessed_file)\n",
    "        df_old_links = pd.read_excel(existing_data, sheet_name=\"Unprocessed Links\")\n",
    "        df_unprocessed_links = pd.concat([df_old_links, df_unprocessed_links], ignore_index=True)\n",
    "\n",
    "    df_unprocessed_links.to_excel(unprocessed_file, sheet_name=\"Unprocessed Links\", index=False)\n",
    "\n",
    "    print(f\"Data saved in '{folder_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ab4849e-464e-445b-a9f4-6592cf272995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 21...\n",
      "Scraped property: https://www.99acres.com/studio-apartment-flat-for-rent-in-divyam-apartments-ulsoor-bangalore-central-120-sq-ft-r1-spid-P77468985\n",
      "Scraped property: https://www.99acres.com/2-bhk-bedroom-independent-builder-floor-for-rent-in-kamraj-road-bangalore-central-1200-sq-ft-spid-J78623779\n",
      "Scraped property: https://www.99acres.com/studio-apartment-flat-for-rent-in-brigade-road-bangalore-central-250-sq-ft-spid-J78502119\n",
      "Scraped property: https://www.99acres.com/2-bhk-bedroom-independent-builder-floor-for-rent-in-sudhama-nagar-bangalore-central-500-sq-ft-spid-Y78317321\n",
      "Scraped property: https://www.99acres.com/2-bhk-bedroom-apartment-flat-for-rent-in-langford-and-richmond-town-richmond-town-bangalore-central-1000-sq-ft-r2-spid-E60729090\n",
      "Scraped property: https://www.99acres.com/1-bhk-bedroom-independent-builder-floor-for-rent-in-ulsoor-bangalore-central-450-sq-ft-r1-spid-R76861511\n",
      "Scraped property: https://www.99acres.com/1-bhk-bedroom-independent-builder-floor-for-rent-in-residency-road-bangalore-central-540-sq-ft-spid-E77820711\n",
      "Scraped property: https://www.99acres.com/1-bhk-bedroom-independent-house-villa-for-rent-in-halasuru-bangalore-central-400-sq-ft-r1-spid-J75525219\n",
      "Scraped property: https://www.99acres.com/2-bhk-bedroom-apartment-flat-for-rent-in-mudamma-garden-bangalore-central-600-sq-ft-spid-B75870941\n",
      "Scraped property: https://www.99acres.com/1-bhk-bedroom-independent-house-villa-for-rent-in-richmond-road-bangalore-central-600-sq-ft-r2-spid-A69559340\n",
      "Data saved in '99acres_Bangalore_Central_scraped_data'\n"
     ]
    }
   ],
   "source": [
    "scrape_99acres(21,21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eefdc31-1039-47fa-9b60-26e529fbe3de",
   "metadata": {},
   "source": [
    "### Description:\n",
    "This script retries scraping rental property listings from 99acres.com that were previously unprocessed. \n",
    "It reads unprocessed links from an Excel file, extracts property details using Selenium and BeautifulSoup, \n",
    "and appends the collected data to the corresponding page file from the initial scraping attempt.\n",
    "\n",
    "### Key functionalities:\n",
    "- Reads unprocessed property links and their corresponding page numbers from an Excel file.\n",
    "- Uses a headless Chrome browser to scrape data.\n",
    "- Extracts various property details (e.g., area, rent, furnishing, location).\n",
    "- Handles missing data with a `safe_extract` function.\n",
    "- Appends newly scraped data to the same page file it originally belonged to.\n",
    "- Updates the unprocessed links file with any links that still fail to load.\n",
    "- Introduces random delays to mimic human behavior and avoid detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "359056af-b499-446d-91ac-3558f8baf218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_Unprocessed_links(folder_name):\n",
    "    \n",
    "    folder_name = folder_name\n",
    "\n",
    "    unprocessed_file = os.path.join(folder_name, \"99acres_unprocessed.xlsx\")\n",
    "\n",
    "    df = pd.read_excel(unprocessed_file)\n",
    "    page_numbers = df.iloc[:, 0]\n",
    "    links = df.iloc[:, 1]\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(90, 110)}.0.0.0 Safari/537.36\")\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    unprocessed_links = []\n",
    "\n",
    "    for i in range(len(links)):\n",
    "        page_data = []\n",
    "        try:\n",
    "            driver.get(links[i])\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"html\")))\n",
    "            time.sleep(random.uniform(4, 7)) \n",
    "\n",
    "            new_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "            def safe_extract(soup, tag, attr_type, attr_value):\n",
    "                try:\n",
    "                    if attr_type == \"id\":\n",
    "                        element = soup.find(tag, id=attr_value)\n",
    "                    elif attr_type == \"class\":\n",
    "                        element = soup.find(tag, class_=attr_value)\n",
    "                    elif attr_type == \"all_class\":\n",
    "                        elements = soup.find_all(tag, class_=attr_value)\n",
    "                        return [item.text.strip() for item in elements]\n",
    "                    return element.text.strip() if element else np.nan\n",
    "                except:\n",
    "                    return np.nan\n",
    "\n",
    "            area_tags = ['superArea_span', 'superbuiltupArea_span', 'builtupArea_span', 'carpetArea_span']\n",
    "            area = ''\n",
    "            for tag in area_tags:\n",
    "                area = safe_extract(new_soup, 'span', 'id', tag)\n",
    "                if (area != '') & (pd.notna(area)):\n",
    "                    break\n",
    "                    \n",
    "            available_from = new_soup.find_all('div' , class_ = 'component__details')\n",
    "\n",
    "            floor = safe_extract(new_soup, 'span', 'id', 'Floor_Num_Label')\n",
    "\n",
    "            page_data.append({\n",
    "                \"Link\": links[i],\n",
    "                \"Bedroom\": safe_extract(new_soup, 'span', 'id', 'bedRoomNum'),\n",
    "                \"Bathroom\": safe_extract(new_soup, 'span', 'id', 'bathroomNum'),\n",
    "                \"Balcony\": safe_extract(new_soup, 'span', 'id', 'balconyNum'),\n",
    "                \"Additional_rooms\": safe_extract(new_soup, 'span', 'id', 'additionalRooms'),\n",
    "                \"Area\": area,\n",
    "                \"Facing\": safe_extract(new_soup, 'span', 'id', 'Facing_Label'),\n",
    "                \"Furnishing\": safe_extract(new_soup, 'span', 'id', 'furnishingLabel'),\n",
    "                \"Rating\": safe_extract(new_soup, 'div', 'class', 'display_l_semiBold'),\n",
    "                \"Address\": safe_extract(new_soup, 'span', 'class', 'component__pdPropAddress'),\n",
    "                \"Nearby\": safe_extract(new_soup, 'span', 'all_class', 'NearByLocation__infoText'),\n",
    "                \"Power_backup\": safe_extract(new_soup, 'span', 'id', 'Powerbackup_Label'),\n",
    "                \"Parking\": safe_extract(new_soup, 'span', 'id', 'Reserved_Parking_Label'),\n",
    "                \"Charges\": safe_extract(new_soup, 'span', 'id', 'electricityWaterCharges'),\n",
    "                \"Posted_By_and_On\": safe_extract(new_soup,'span', 'id', 'postedOnAndByLabel'),\n",
    "                \"Age\": safe_extract(new_soup, 'span', 'id', 'Age_Label'),\n",
    "                \"Pet_friendly\": safe_extract(new_soup, 'span', 'id', 'PetFriendly'),\n",
    "                \"Floor\": safe_extract(new_soup , 'span' , 'id' , 'Total_Floor') if pd.isnull(floor) else floor,\n",
    "                \"Avaliable_from\": available_from[len(available_from)-2].text.strip(),\n",
    "                \"Available_for\": safe_extract(new_soup, 'span', 'id', 'availableForLabel'),\n",
    "                \"Type\": safe_extract(new_soup, 'div', 'class', 'component__pdPropConfSide component__rentHeading pd__rentHeading'),\n",
    "                \"Advance\": safe_extract(new_soup, 'div', 'class', 'component__tableTooltip'),\n",
    "                \"Location\": safe_extract(new_soup, 'span', 'class', 'component__pdPropAddress'),\n",
    "                \"Rent\": safe_extract(new_soup, 'span', 'id', 'pdPrice2')\n",
    "            })\n",
    "\n",
    "            print(f\"Scraped property: {links[i]}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing link {links[i]}: {e}\")\n",
    "            unprocessed_links.append({\"Page\": page_numbers[i], \"Link\": links[i]})\n",
    "            continue\n",
    "\n",
    "        time.sleep(random.uniform(5, 8))\n",
    "        df_page = pd.DataFrame(page_data)\n",
    "        file_name = os.path.join(folder_name, f\"99acres_rentals_page_{page_numbers[i]}.xlsx\")\n",
    "        \n",
    "        if os.path.exists(file_name):\n",
    "            df_old = pd.read_excel(file_name)\n",
    "            df_page = pd.concat([df_old, df_page], ignore_index=True)\n",
    "        \n",
    "        df_page.to_excel(file_name, index=False)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    df_unprocessed_links = pd.DataFrame(unprocessed_links)\n",
    "    df_unprocessed_links.to_excel(unprocessed_file, sheet_name=\"Unprocessed Links\", index=False)\n",
    "\n",
    "    print(f\"Data saved in '{folder_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c81b2a-bd70-47ff-bc31-7b49c1990ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped property: https://www.99acres.com/3-bhk-bedroom-apartment-flat-for-rent-in-cooke-town-bangalore-central-1750-sq-ft-r1-spid-E78896081\n",
      "Scraped property: https://www.99acres.com/2-bhk-bedroom-apartment-flat-for-rent-in-deepa-residency-someshwarpura-bangalore-central-1100-sq-ft-spid-K80075533\n",
      "Data saved in '99acres_Bangalore_Central_scraped_data'\n"
     ]
    }
   ],
   "source": [
    "scrape_Unprocessed_links(\"99acres_Bangalore_Central_scraped_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
